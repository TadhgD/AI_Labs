{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Spam filters</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import linear_model\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Starting off</h1>\n",
    "I started by computing how the majority classifier would fare on this dataset, so I would have some marker to go off of. The majority classifier always predicts the most prominent class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1248\n",
      "1650\n",
      "0.5693581780538303\n"
     ]
    }
   ],
   "source": [
    "len_spam_folder = len(os.listdir(\"spam\"))\n",
    "len_ham_folder  = len(os.listdir(\"ham\"))\n",
    "print(len_spam_folder)\n",
    "print(len_ham_folder)\n",
    "majority_c = len_ham_folder / (len_spam_folder + len_ham_folder)\n",
    "print(majority_c) #Majority classifier will be correct ~56.9% of the time (predicts 'ham')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Retrieving the email files</h1>\n",
    "I then went about creating a dataframe containing all of the emails using a simple loop. Then, I shuffled the dataframe to intermingle the spam and ham examples. I do this so that when using error estimation later on, that kfolds will be operating off of a randomized list and not the spam and then ham examples in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_vectors = []\n",
    "dirs = [\"spam\", \"ham\"]\n",
    "for directory in dirs:\n",
    "    all_files = os.listdir(directory)\n",
    "    for file in all_files:\n",
    "        email = open(directory + \"/\" + file, encoding=\"utf8\")\n",
    "        content = email.read()\n",
    "        vector = {\"y_value\": [directory], \"content\": [content]}\n",
    "        email_vector = pd.DataFrame(data=vector)\n",
    "        email_vectors.append(email_vector)\n",
    "        email.close()\n",
    "        \n",
    "df = pd.concat(email_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.take(np.random.permutation(len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Applying vectorizers</h1>\n",
    "To compare Count Vectorization and Tfidf Vectorization, I created a pipeline for each and transformed the data, creating two new dataframes.\n",
    "Count Vectorization turns each tokenized word into a feature, with the feature values being the frequencies of these words for each example. This results in quite a lot of features. In an effort to reduce the number of features, I specified to not tokenize any stop words.\n",
    "Tfidf works similarly, where instead of word frequencies, a Tfidf score is used instead. This Tfidf score increases proportionally to the frequency of a word in a particular document, but is also offset by the frequency of the frequency of the word in the corpus. I took the same action on the stop words with this as I did for Count Vectorization.\n",
    "\n",
    "Tfidf definition taken from wikipedia :\n",
    "https://en.wikipedia.org/wiki/Tf%E2%80%93idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = df['content'] #Take out the email contents.\n",
    "\n",
    "cv_pipeline = Pipeline ([\n",
    "    (\"vectorizer\", CountVectorizer(stop_words=\"english\")) #Create count vectorizer pipeline.\n",
    "])\n",
    "tfidf_pipeline = Pipeline ([\n",
    "    (\"tfidf\", TfidfVectorizer(stop_words=\"english\")) #Create the Tfidf vectorizer pipeline.\n",
    "])\n",
    "\n",
    "cv_pipeline.fit(content)\n",
    "cv_df = cv_pipeline.transform(content) #Create the Count Vectorizer dataframe.\n",
    "\n",
    "tfidf_pipeline.fit(content)\n",
    "tfidf_df = tfidf_pipeline.transform(content) #Create the Tfidf dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I encoded the y values for use later on in error estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['y_value'].values\n",
    "encoder = LabelEncoder()\n",
    "y_encoded = encoder.fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Create the pipeline and compare accuracy</h1>\n",
    "Here, I create a Logistic Regression pipeline, and then specify to use 10-fold cross validation.\n",
    "\n",
    "I then compare the kfolds error values of the Count Vectorizer model vs. the Tfidf Vectorizer model.\n",
    "\n",
    "I solely used kfolds for error estimation and neglected to use holdout as I believe the dataset is too small for holdout. In the lectures, it was mentioned that the dataset would have to be very large to mitigate holdouts flaws, such as sub-optimal use of the dataset for training. Therefore, I decided to stick with kfolds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorization accuracy:  0.980674143897\n",
      "\n",
      "\n",
      "Tfidf accuracy              :  0.954106908483\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    (\"estimator\", LogisticRegression())\n",
    "])\n",
    "\n",
    "cv_accuracy = np.mean(cross_val_score(pipeline, cv_df, y_encoded, scoring=\"accuracy\", cv=10))\n",
    "\n",
    "tfidf_accuracy = np.mean(cross_val_score(pipeline, tfidf_df, y_encoded, scoring=\"accuracy\", cv=10))\n",
    "\n",
    "print(\"Count Vectorization accuracy: \", cv_accuracy)\n",
    "print(\"\\n\")\n",
    "print(\"Tfidf accuracy              : \", tfidf_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Analysis</h1>\n",
    "From the results above, it would appear that Count Vectorization is the better method to go with.\n",
    "\n",
    "I decided to take a look at the confusion matrices of both, to see the rate how many False Positives and False Negatives occurred within each. In the case of a Spam filter, False Positives are much worse, so I looked at these with particular scrutiny.\n",
    "\n",
    "While in terms of accuracy Tfidf is usually ~2% worse than Count Vectorization, it usually has about twice as many false positives, which arguably makes it twice as bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorization Confusion Matrix:\n",
      "[[1622   28]\n",
      " [  28 1220]]\n",
      "\n",
      "\n",
      "Tfidf Confusion Matrix:\n",
      "[[1584   66]\n",
      " [  69 1179]]\n"
     ]
    }
   ],
   "source": [
    "cv_y_predicted = cross_val_predict(pipeline, cv_df, y_encoded, cv=kf)\n",
    "\n",
    "cv_conf_matrix = confusion_matrix(y_encoded, cv_y_predicted)\n",
    "print(\"Count Vectorization Confusion Matrix:\")\n",
    "print(cv_conf_matrix)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "tfidf_y_predicted = cross_val_predict(pipeline, tfidf_df, y_encoded, cv=kf)\n",
    "\n",
    "tfidf_conf_matrix = confusion_matrix(y_encoded, tfidf_y_predicted)\n",
    "print(\"Tfidf Confusion Matrix:\")\n",
    "print(tfidf_conf_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Extra work</h1>\n",
    "I searched online for a bit of extra reading that might give me some good ideas for what else to try. I stumbled across an interesting article by a man named Paul Graham, which gave me a few ideas. The article was based on a talk he gave at the 2003 Spam conference about Bayesian Filtering.\n",
    "http://www.paulgraham.com/better.html\n",
    "\n",
    "<h2>Preserving case</h2>\n",
    "For example, he discusses how important preserving case is for detecting spam. I tried to implement this in the CountVectorizer pipeline, but for some reason the number of features didn't change, even when I ran it by itself before running the count vectorizer I created above.\n",
    "If implemented properly, although it would increase the number of features, it would be interesting to see the effect it would have in reducing the efficacy of the spam filter. I would have compared this to the plain Count Vectorizer filter and paid particular attention to the False Positives number again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = df['content'] #Take out the email contents.\n",
    "\n",
    "cv_pipeline_case_preserved = Pipeline ([\n",
    "    (\"vectorizer\", CountVectorizer(lowercase=\"False\",stop_words=\"english\")) \n",
    "])\n",
    "\n",
    "cv_pipeline_case_preserved.fit(content)\n",
    "cv_casep_df = cv_pipeline_case_preserved.transform(content) #Create the Count Vectorizer dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2898, 109449)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_casep_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.979986875075\n"
     ]
    }
   ],
   "source": [
    "cv_casep_accuracy = np.mean(cross_val_score(pipeline, cv_casep_df, y_encoded, scoring=\"accuracy\", cv=kf))\n",
    "print(cv_casep_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Reducing features</h2>\n",
    "\n",
    "Paul Graham discussed how having more features in your dataset is akin to having a smaller corpus, so I attempted to try and reduce the number of features.\n",
    "\n",
    "In an attempt to reduce the number of features in, I tried to use maxdf, and also SVD. \n",
    "SVD was a bit naive, as I wouldn't have been able to map any y values to the transformed values anyway. In the end, it didn't work, as I kept getting memory errors.\n",
    "\n",
    "For max_df, I went as far as to reduce ~100 features, but this ended up nuking the accuracy of model. Hence, I concluded that any reduction in features in this way to be pointless, as the accuracy tradeoff was too great.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = df['content']\n",
    "\n",
    "cv_svd_pipeline = Pipeline ([\n",
    "    (\"vectorizer\", CountVectorizer(stop_words=\"english\",max_df=0.9)) #Create count vectorizer pipeline.\n",
    "])\n",
    "\n",
    "#cv_svd_pipeline.fit(content)\n",
    "#cv_svd_df = cv_svd_pipeline.transform(content)\n",
    "\n",
    "\n",
    "#svd = TruncatedSVD(n_components=10000)\n",
    "#svd.fit(cv_df)\n",
    "\n",
    "cv_max_df_pipeline = Pipeline ([\n",
    "    (\"vectorizer\", CountVectorizer(stop_words=\"english\",max_df=0.5)) #Create count vectorizer pipeline.\n",
    "])\n",
    "\n",
    "cv_max_df_pipeline.fit(content)\n",
    "cv_max_df_df = cv_max_df_pipeline.transform(content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2898, 109387)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_max_df_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_df_pipeline = Pipeline([\n",
    "    (\"estimator\", LogisticRegression())\n",
    "])\n",
    "\n",
    "cv_max_df_accuracy = np.mean(cross_val_score(max_df_pipeline, cv_max_df_df, y_encoded, scoring=\"accuracy\", cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.514858608758\n"
     ]
    }
   ],
   "source": [
    "print(cv_max_df_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
